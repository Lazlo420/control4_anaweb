{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "reemplazo_acentos = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u'}\n",
    "#nltk.download(\"punkt\")\n",
    "#cargo los stopwords\n",
    "with open(\"spanish.txt\",\"r\",encoding=\"utf-8\") as sp:\n",
    "    stop_list=[word.strip() for word in sp.readlines()]\n",
    "#cargo el libro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones\n",
    "\n",
    "No uso todas. Creo que la mejor solucion es con objetos alucina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(strp):\n",
    "    limpio = re.sub(r'[áéíóú]', lambda x: reemplazo_acentos[x.group()], strp)\n",
    "    # Tokenización y eliminación de puntuación\n",
    "    tokens = nltk.word_tokenize(re.sub(r'[^\\w\\s]', '', limpio), language=\"spanish\")\n",
    "    tokens=[token for token in tokens if token not in stop_list]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vector():\n",
    "    def __init__(self, corpus) -> None:\n",
    "\n",
    "        self.corpus = list(corpus.values())\n",
    "        self.files= {x:i for i,x in enumerate(list(corpus.keys()))}\n",
    "\n",
    "        self.dicc = {}\n",
    "        self.set_dic()\n",
    "\n",
    "        self.n_doc = len(self.files)\n",
    "        self.n_terms = len(self.dicc)\n",
    "\n",
    "        self.tf = np.zeros((self.n_doc, self.n_terms), dtype=int)\n",
    "        self.idf = np.zeros(self.n_terms)\n",
    "        self.tf_idf = np.zeros((self.n_doc, self.n_terms), dtype=float)\n",
    "        self.tf_idf_keys={}    \n",
    "\n",
    "    def set_dic(self):\n",
    "        tempdic=[]\n",
    "        for v in self.corpus:\n",
    "            tempdic.extend(v)\n",
    "        self.dicc={x:i for i, x in enumerate(set(tempdic))}\n",
    "        \n",
    "    def get_idf(self):\n",
    "        self.idf = np.count_nonzero(self.tf, axis=0)\n",
    "\n",
    "    def get_tf_idf(self):\n",
    "        self.tf_idf=self.tf*np.log2(self.n_doc/self.idf)\n",
    "\n",
    "    def get_tf(self,mode='glob',topn = False, minv = False):\n",
    "        res_dic=[]       \n",
    "        #para cada elemento en corpus \n",
    "        for i,v in enumerate(self.corpus):\n",
    "            #llevamos cuenta de las palabras. Siguiendo el diccionario general\n",
    "            dicc=dict.fromkeys(self.dicc.keys(), 0)\n",
    "\n",
    "            #contea las palabras en corpus\n",
    "            for word in v:\n",
    "                dicc[word]=dicc.get(word,0) + 1\n",
    "            \n",
    "            #crea el array\n",
    "            conteo=np.array(list(dicc.values()))\n",
    "\n",
    "            match mode:\n",
    "                case 'glob':\n",
    "                    pass\n",
    "\n",
    "                case 'topn':\n",
    "                    #como hacemos en el ordenado\n",
    "                    #filtramos\n",
    "                    dicc=dict(sorted(dicc.items(), key=lambda item: item[1],reverse=True))\n",
    "                    dicc=list(dicc)[:topn]\n",
    "                    res_dic.extend(dicc)\n",
    "\n",
    "                case 'minv':\n",
    "                    dicc={k:v for k,v in dicc.items() if v>minv}\n",
    "                    dicc=list(dicc)\n",
    "                    res_dic.extend(dicc)\n",
    "                    #filtramos, pero ahora minv\n",
    "                    \n",
    "            self.tf[i]=conteo\n",
    "\n",
    "        if len(res_dic)!=0:\n",
    "            res_dic=list(set(res_dic))\n",
    "            indx=[self.dicc.get(i,0) for i in res_dic]\n",
    "            self.tf=self.tf[:,indx]\n",
    "            self.tf_idf_keys={i:x for x,i in enumerate(res_dic)}\n",
    "\n",
    "        self.get_idf()\n",
    "        self.get_tf_idf()\n",
    "            \n",
    "    def get_vec(self, case, files= None):\n",
    "        if files==None:\n",
    "            fil=list(self.files.values())\n",
    "        else:\n",
    "            fil=[self.files.get(x,0) for x in files]\n",
    "        match case:\n",
    "            case 'tf':\n",
    "                vector=np.mean(self.tf[fil,:],axis=0)\n",
    "            case 'tf-idf':\n",
    "                vector=np.mean(self.tf_idf[fil,:],axis=0)\n",
    "\n",
    "        return vector\n",
    "    \n",
    "    def new_get_vec(self,file,mode='tf'):\n",
    "        dicc=dict.fromkeys(self.tf_idf_keys.keys(),0)\n",
    "        for word in file:\n",
    "            if word in dicc.keys():\n",
    "                dicc[word]=dicc.get(word,0)+1\n",
    "        dicc=np.array(list(dicc.values()))\n",
    "        match mode:\n",
    "            case 'tf':\n",
    "                return dicc\n",
    "            case 'tf-idf':\n",
    "                tf_idf=dicc*np.log2(self.n_doc/self.idf)\n",
    "                return tf_idf\n",
    "    \n",
    "    def sim_cor(self,file,vec_files,mode='tf'):\n",
    "        vect=self.get_vec(mode,vec_files)\n",
    "        comp=self.new_get_vec(file, mode)\n",
    "        distance = cosine(comp,vect)\n",
    "        return distance \n",
    "\n",
    "    def show_topn(self,topn,files=None):\n",
    "        if files==None:\n",
    "            fil=list(self.files.values())\n",
    "        else:\n",
    "            fil=[self.files.get(x,0) for x in files]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargo archivos, separo grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "compendio={}\n",
    "articulos=[\"articulos/\"+ruta for ruta in os.listdir('articulos')]\n",
    "\n",
    "for articulo in articulos:\n",
    "    with open(articulo,\"r\",encoding=\"utf-8\") as b:\n",
    "        strp=b.read().lower()\n",
    "    strp=prepro(strp)\n",
    "    compendio[articulo.replace(\"articulos/\",'')]=strp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lebron=compendio.pop('NBA.txt')\n",
    "test_mgs=compendio.pop('Videojuego_de_sigilo.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lebron=['Pie_(unidad).txt',\n",
    "        'Ohio.txt',\n",
    "        'Akron.txt',\n",
    "        'Baloncesto.txt',\n",
    "        'Cleveland_Cavaliers.txt',\n",
    "        'Draft_de_la_NBA.txt',\n",
    "        'Draft_de_la_NBA_de_2003.txt',\n",
    "        'Estados_Unidos.txt',\n",
    "        'Kilogramo.txt',\n",
    "        'LeBron_James.txt']\n",
    "\n",
    "mgs=['Entertainment_Software_Rating_Board.txt',\n",
    "     'Hideo_Kojima.txt',\n",
    "     'Konami.txt',\n",
    "     'Metal_Gear_Solid.txt',\n",
    "     'Metal_Gear_Solid_Digital_Graphic_Novel.txt',\n",
    "     'Metal_Gear_Solid__The_Twin_Snakes.txt',\n",
    "     'Microsoft.txt',\n",
    "     'Pan_European_Game_Information.txt',\n",
    "     'Videojuego_de_t%C3%A1ctica_en_tiempo_real.txt',\n",
    "     'Videojuego_de_un_jugador.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas y vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF, TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = Vector(compendio)\n",
    "vector.get_tf(mode='topn',topn=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tiempo': 0,\n",
       " 'tras': 1,\n",
       " 'snake': 2,\n",
       " 'entertainment': 3,\n",
       " 'estadounidenses': 4,\n",
       " 'ser': 5,\n",
       " 'konami': 6,\n",
       " 'aumento': 7,\n",
       " 'software': 8,\n",
       " 'normalmente': 9,\n",
       " 'metanfetamina': 10,\n",
       " 'siglo': 11,\n",
       " 'temporada': 12,\n",
       " 'estadounidense': 13,\n",
       " 'poblacion': 14,\n",
       " 'version': 15,\n",
       " 'historia': 16,\n",
       " '2': 17,\n",
       " 'generalmente': 18,\n",
       " 'unidad': 19,\n",
       " 'playstation': 20,\n",
       " 'ciudad': 21,\n",
       " 'equipos': 22,\n",
       " 'cerca': 23,\n",
       " 'jugador': 24,\n",
       " 'dado': 25,\n",
       " 'metal': 26,\n",
       " 'descriptores': 27,\n",
       " 'baloncesto': 28,\n",
       " 'coherentes': 29,\n",
       " 'ganador': 30,\n",
       " 'cm': 31,\n",
       " 'sistema': 32,\n",
       " 'twin': 33,\n",
       " 'kojima': 34,\n",
       " 'juegos': 35,\n",
       " 'unidos': 36,\n",
       " 'cleveland': 37,\n",
       " 'eleccion': 38,\n",
       " 'videojuegos': 39,\n",
       " 'romano': 40,\n",
       " 'association': 41,\n",
       " 'm': 42,\n",
       " 'jugadores': 43,\n",
       " 'esrb': 44,\n",
       " 'videojuego': 45,\n",
       " 'millones': 46,\n",
       " 'windows': 47,\n",
       " 'region': 48,\n",
       " 'reglas': 49,\n",
       " 'lebron': 50,\n",
       " 'c': 51,\n",
       " 'pie': 52,\n",
       " 'snakes': 53,\n",
       " 'estrategia': 54,\n",
       " 'kilogramo': 55,\n",
       " 'nba': 56,\n",
       " 'portatil': 57,\n",
       " 'triples': 58,\n",
       " 'rebotes': 59,\n",
       " 'h': 60,\n",
       " 'practicas': 61,\n",
       " 'nintendo': 62,\n",
       " 'nombre': 63,\n",
       " 'reclamaciones': 64,\n",
       " 'aproximadamente': 65,\n",
       " 'pais': 66,\n",
       " 'agregada': 67,\n",
       " 'chris': 68,\n",
       " 'unidades': 69,\n",
       " 'akron': 70,\n",
       " 'cavs': 71,\n",
       " 'batalla': 72,\n",
       " 'solid': 73,\n",
       " 'gear': 74,\n",
       " 'comic': 75,\n",
       " 'puede': 76,\n",
       " 'gamecube': 77,\n",
       " 'mejores': 78,\n",
       " 'solo': 79,\n",
       " 'real': 80,\n",
       " 'draft': 81,\n",
       " 'tacticas': 82,\n",
       " 'masa': 83,\n",
       " 'contenido': 84,\n",
       " 'ohio': 85,\n",
       " 'años': 86,\n",
       " 'puntos': 87,\n",
       " 'primera': 88,\n",
       " 'condado': 89,\n",
       " 'partido': 90,\n",
       " 'edades': 91,\n",
       " 'si': 92,\n",
       " 'deporte': 93,\n",
       " 'juego': 94,\n",
       " 'tambien': 95,\n",
       " 'nueva': 96,\n",
       " 'james': 97,\n",
       " 'consejo': 98,\n",
       " 'equipo': 99,\n",
       " 'asistencias': 100,\n",
       " 'original': 101,\n",
       " 'mayor': 102,\n",
       " '000': 103,\n",
       " 'mm': 104,\n",
       " 'tactica': 105,\n",
       " 'conferencia': 106,\n",
       " 'japon': 107,\n",
       " 'finales': 108,\n",
       " 'balon': 109,\n",
       " 'canasta': 110,\n",
       " 'empresa': 111,\n",
       " 'parte': 112,\n",
       " 'digital': 113,\n",
       " 'productos': 114,\n",
       " 'ltd': 115,\n",
       " 'clasificacion': 116,\n",
       " 'talentosos': 117,\n",
       " 'the': 118,\n",
       " 'microsoft': 119,\n",
       " 'cavaliers': 120,\n",
       " 'jugabilidad': 121,\n",
       " 'compañia': 122,\n",
       " 'paises': 123,\n",
       " 'nuevo': 124,\n",
       " 'internacional': 125,\n",
       " '2007': 126,\n",
       " 'guerra': 127,\n",
       " 'longitud': 128,\n",
       " 'km²': 129,\n",
       " 'modo': 130,\n",
       " 'año': 131,\n",
       " 'pegi': 132,\n",
       " 'goma': 133,\n",
       " 'personas': 134,\n",
       " 'elegibles': 135,\n",
       " 'clasificaciones': 136,\n",
       " 'magnitudes': 137,\n",
       " 'medida': 138,\n",
       " 'playoffs': 139,\n",
       " 'gran': 140}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.tf_idf_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec as w2v\n",
    "def article_vector(article):\n",
    "    vectors = [modelo.wv[word] for word in article if word in modelo.wv]\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=list(compendio.values())\n",
    "modelo = w2v(sentences=sentences,vector_size=100,window=5,min_count=5,epochs=5,workers=6)\n",
    "\n",
    "test_lebron_vector=article_vector(test_lebron)\n",
    "test_mgs_vector=article_vector(test_mgs)\n",
    "\n",
    "grupo_lebron={k:v for k,v in compendio.items() if k in lebron}\n",
    "grupo_mgs={k:v for k,v in compendio.items() if k in mgs}\n",
    "\n",
    "# Obtener representación vectorial de los artículos en grupo1 y grupo2\n",
    "grupo_lebron_vectors = [article_vector(article) for article in grupo_lebron.values()]\n",
    "grupo_mgs_vectors = [article_vector(article) for article in grupo_mgs.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i1.sndcdn.com/artworks-9vp6lBh9T65XVu1X-xMPTjg-t500x500.jpg\" alt=\"drawing\" width=\"200\"/> <img src=\"https://w7.pngwing.com/pngs/867/35/png-transparent-arrow-right-face-funny-expressions-blue-pointing-pointers-directions-happy.png\" alt=\"drawing\" width=\"200\"/> <img src=\"https://media.tenor.com/qBNk0a044ogAAAAe/metal-gear-solid-otacon.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "\n",
    "**Lebron** en **Lebron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distancia coseno (TF): 0.4412928211916993\n",
      "Distancia coseno (TF-IDF): 0.5528261901460279\n",
      "Distancia coseno (w2v): 1.6084563919183116e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Distancia coseno (TF): {vector.sim_cor(test_lebron,vec_files=lebron,mode='tf')}\")\n",
    "print(f\"Distancia coseno (TF-IDF): {vector.sim_cor(test_lebron,vec_files=lebron,mode='tf-idf')}\")\n",
    "\n",
    "similarity_grupo_lebron = np.mean([cosine(test_lebron_vector, vector) for vector in grupo_lebron_vectors])\n",
    "print(\"Distancia coseno (w2v):\", similarity_grupo_lebron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lebron** en **Metal Gear**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distancia coseno (TF): 0.863928179709584\n",
      "Distancia coseno (TF-IDF): 0.9765047928755942\n",
      "Distancia coseno (w2v) 5.18542551905643e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Distancia coseno (TF): {vector.sim_cor(test_lebron,vec_files=mgs,mode='tf')}\")\n",
    "print(f\"Distancia coseno (TF-IDF): {vector.sim_cor(test_lebron,vec_files=mgs,mode='tf-idf')}\")\n",
    "similarity_grupo_mgs = np.mean([cosine(test_lebron_vector, vector) for vector in grupo_mgs_vectors])\n",
    "print(\"Distancia coseno (w2v)\", similarity_grupo_mgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metal Gear** en **Lebron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distancia coseno (TF): 0.7895209521472933\n",
      "Distancia coseno (TF-IDF): 0.9420598612554753\n",
      "Distancia coseno (w2v): 1.6084563919183116e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Distancia coseno (TF): {vector.sim_cor(test_mgs,vec_files=lebron,mode='tf')}\")\n",
    "print(f\"Distancia coseno (TF-IDF): {vector.sim_cor(test_mgs,vec_files=lebron,mode='tf-idf')}\")\n",
    "\n",
    "similarity_grupo_mgs = np.mean([cosine(test_mgs_vector, vector) for vector in grupo_lebron_vectors])\n",
    "print(\"Distancia coseno (w2v):\", similarity_grupo_lebron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metal Gear** en **Metal Gear**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distancia coseno (TF): 0.3704846228420089\n",
      "Distancia coseno (TF-IDF): 0.5358710096109732\n",
      "Distancia coseno (w2v): 1.6084563919183116e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Distancia coseno (TF): {vector.sim_cor(test_mgs,vec_files=mgs,mode='tf')}\")\n",
    "print(f\"Distancia coseno (TF-IDF): {vector.sim_cor(test_mgs,vec_files=mgs,mode='tf-idf')}\")\n",
    "\n",
    "similarity_grupo_mgs = np.mean([cosine(test_mgs_vector, vector) for vector in grupo_mgs_vectors])\n",
    "print(\"Distancia coseno (w2v):\", similarity_grupo_lebron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretacion\n",
    "\n",
    "Intente colocar una al lado de otra, pero no pude (aun no se interpretar)\n",
    "\n",
    "| TF                | Lebron   | MGS   |\n",
    "| :---------------- | :------: | ----: |\n",
    "| Lebron            |   0.44   | 0.86 |\n",
    "| MGS               |   0.79   | 0.37 |\n",
    "\n",
    "\n",
    "\n",
    "| TF-IDF            | Lebron   | MGS   |\n",
    "| :---------------- | :------: | ----: |\n",
    "| Lebron            |   0.55   | 0.98 |\n",
    "| MGS               |   0.94   | 0.54 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| W2V                | Lebron   | MGS   |\n",
    "| :---------------- | :------: | ----: |\n",
    "| Lebron            |   1.60e-05   | 5.18e-05 |\n",
    "| MGS               |   1.60e-05   | 1.60e-05 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
